{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:47:55.562339Z",
     "start_time": "2020-01-24T17:47:54.283805Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "pd.set_option('max_rows', 1000)\n",
    "pd.set_option('max_columns', 1000)\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)\n",
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:47:55.646926Z",
     "start_time": "2020-01-24T17:47:55.563940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (80402, 6)\n",
      "test shape: (180, 3)\n",
      "sample submission shape: (180, 4)\n",
      "\n",
      "\n",
      "train date min: 2017-04-01 00:00:00\n",
      "train date max: 2019-07-23 00:00:00\n",
      "test date min: 2019-07-06 00:00:00\n",
      "test date max: 2019-10-24 00:00:00\n",
      "\n",
      "\n",
      "train date min (segment1): 2017-04-01 00:00:00\n",
      "train date max (segment1): 2019-07-05 00:00:00\n",
      "train date min (segment2): 2017-04-01 00:00:00\n",
      "train date max (segment2): 2019-07-23 00:00:00\n",
      "\n",
      "\n",
      "test date min (segment 1): 2019-07-06 00:00:00\n",
      "test date max (segment 1): 2019-09-30 00:00:00\n",
      "test date min (segment 2): 2019-07-24 00:00:00\n",
      "test date max (segment 2): 2019-10-24 00:00:00\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/train_fwYjLYX.csv\", parse_dates = ['application_date'])\n",
    "test = pd.read_csv(\"../input/test_1eLl9Yf.csv\", parse_dates = ['application_date'])\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission_IIzFVsf.csv\")\n",
    "\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "print(f\"sample submission shape: {sample_submission.shape}\")\n",
    "print(\"\\n\")\n",
    "print(f\"train date min: {train['application_date'].min()}\")\n",
    "print(f\"train date max: {train['application_date'].max()}\")\n",
    "print(f\"test date min: {test['application_date'].min()}\")\n",
    "print(f\"test date max: {test['application_date'].max()}\")\n",
    "print(\"\\n\")\n",
    "print(f\"train date min (segment1): {train.query('segment == 1')['application_date'].min()}\")\n",
    "print(f\"train date max (segment1): {train.query('segment == 1')['application_date'].max()}\")\n",
    "print(f\"train date min (segment2): {train.query('segment == 2')['application_date'].min()}\")\n",
    "print(f\"train date max (segment2): {train.query('segment == 2')['application_date'].max()}\")\n",
    "print(\"\\n\")\n",
    "print(f\"test date min (segment 1): {test.query('segment == 1')['application_date'].min()}\")\n",
    "print(f\"test date max (segment 1): {test.query('segment == 1')['application_date'].max()}\")\n",
    "print(f\"test date min (segment 2): {test.query('segment == 2')['application_date'].min()}\")\n",
    "print(f\"test date max (segment 2): {test.query('segment == 2')['application_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:47:55.659325Z",
     "start_time": "2020-01-24T17:47:55.648558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(844, 2)\n"
     ]
    }
   ],
   "source": [
    "seg2 = train.query('segment == 2').groupby(['application_date'])['case_count'].agg('sum').reset_index()\n",
    "seg2.rename(columns = {'application_date': 'ds', 'case_count': 'y'}, inplace =  True)\n",
    "seg2['y'] = np.log1p(seg2['y'])\n",
    "print(seg2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:47:56.964226Z",
     "start_time": "2020-01-24T17:47:56.946389Z"
    }
   },
   "outputs": [],
   "source": [
    "seg2[\"day\"] = pd.Categorical(seg2[\"ds\"].dt.day)\n",
    "seg2[\"month\"] = pd.Categorical(seg2[\"ds\"].dt.month)\n",
    "seg2[\"quarter\"] = pd.Categorical((seg2[\"ds\"].dt.month - 1) // 3)\n",
    "seg2[\"weekday\"] = pd.Categorical(seg2[\"ds\"].dt.weekday)\n",
    "onehot = pd.get_dummies(seg2[[\"day\", \"month\", \"quarter\", \"weekday\"]])\n",
    "seg2 = pd.concat([seg2, onehot], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:47:59.596574Z",
     "start_time": "2020-01-24T17:47:59.096645Z"
    }
   },
   "outputs": [],
   "source": [
    "holiday_df = scrape_national_holidays()\n",
    "holiday_df = holiday_df[holiday_df.holiday != 'Buddha Purnima/Vesak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:48:00.244277Z",
     "start_time": "2020-01-24T17:48:00.223312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using monthly seasonality\n",
      "using weekly seasonality\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'holiday': True,\n",
    "    'holidays_prior_scale': 0.01,\n",
    "    'changepoint_prior_scale': 0.03,\n",
    "    'seasonal': {\n",
    "        'monthly': (30, 1, 0.1, 'additive'),\n",
    "        'weekly': (7, 1, 0.55, 'additive'),\n",
    "    }\n",
    "}\n",
    "m = build_aggregated_model(params = params, verbose = True, holiday_df=holiday_df)\n",
    "\n",
    "for col in seg2.columns:\n",
    "    if col not in ['y', 'ds', 'day', 'quarter', 'month', 'weekday']:\n",
    "        if 'day' in col:\n",
    "            day = int(col.split(\"_\")[1])\n",
    "            if day == 1:\n",
    "                m.add_regressor(col, prior_scale = 0.1 , mode = 'additive')\n",
    "            else:\n",
    "                m.add_regressor(col, prior_scale = 0.1, mode = 'additive')\n",
    "        elif 'week' in col:\n",
    "            m.add_regressor(col, prior_scale = 1, mode = 'additive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:48:11.599374Z",
     "start_time": "2020-01-24T17:48:11.579202Z"
    }
   },
   "outputs": [],
   "source": [
    "seg2_test = test.query('segment == 2')[['application_date']].rename(columns = {'application_date': 'ds'})\n",
    "seg2_test[\"day\"] = pd.Categorical(seg2_test[\"ds\"].dt.day)\n",
    "seg2_test[\"month\"] = pd.Categorical(seg2_test[\"ds\"].dt.month)\n",
    "seg2_test[\"quarter\"] = pd.Categorical((seg2_test[\"ds\"].dt.month - 1) // 3)\n",
    "seg2_test[\"weekday\"] = pd.Categorical(seg2_test[\"ds\"].dt.weekday)\n",
    "onehot = pd.get_dummies(seg2_test[[\"day\", \"month\", \"quarter\", \"weekday\"]])\n",
    "seg2_test = pd.concat([seg2_test, onehot], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:48:36.388087Z",
     "start_time": "2020-01-24T17:48:33.707525Z"
    }
   },
   "outputs": [],
   "source": [
    "m.fit(seg2)\n",
    "fc_seg2 = m.predict(seg2_test)\n",
    "fc_seg2['yhat'] = np.expm1(fc_seg2['yhat'])\n",
    "fc_seg2['yhat'] = np.clip(fc_seg2['yhat'], 0., fc_seg2['yhat'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:48:37.921597Z",
     "start_time": "2020-01-24T17:48:37.906544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806, 2)\n"
     ]
    }
   ],
   "source": [
    "seg1 = train.query('segment == 1').groupby(['application_date'])['case_count'].agg('sum').reset_index()\n",
    "seg1.rename(columns = {'application_date': 'ds', 'case_count': 'y'}, inplace =  True)\n",
    "print(seg1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:48:38.639284Z",
     "start_time": "2020-01-24T17:48:38.622489Z"
    }
   },
   "outputs": [],
   "source": [
    "seg1[\"day\"] = pd.Categorical(seg1[\"ds\"].dt.day)\n",
    "seg1[\"month\"] = pd.Categorical(seg1[\"ds\"].dt.month)\n",
    "seg1[\"quarter\"] = pd.Categorical((seg1[\"ds\"].dt.month - 1) // 3)\n",
    "seg1[\"weekday\"] = pd.Categorical(seg1[\"ds\"].dt.weekday)\n",
    "onehot = pd.get_dummies(seg1[[\"day\", \"month\", \"quarter\", \"weekday\"]])\n",
    "seg1 = pd.concat([seg1, onehot], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:48:40.267925Z",
     "start_time": "2020-01-24T17:48:40.261045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using weekly seasonality\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'holiday': True,\n",
    "    'holidays_prior_scale': 0.1,\n",
    "    'changepoint_prior_scale': .6,\n",
    "    'seasonal': {\n",
    "        'weekly': (7, 5, 0.7, 'additive'),\n",
    "    }\n",
    "}\n",
    "m = build_aggregated_model(params = params, verbose = True, holiday_df=holiday_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:48:41.202675Z",
     "start_time": "2020-01-24T17:48:41.182368Z"
    }
   },
   "outputs": [],
   "source": [
    "seg1_test = test.query('segment == 1')[['application_date']].rename(columns = {'application_date': 'ds'})\n",
    "seg1_test[\"day\"] = pd.Categorical(seg1_test[\"ds\"].dt.day)\n",
    "seg1_test[\"month\"] = pd.Categorical(seg1_test[\"ds\"].dt.month)\n",
    "seg1_test[\"quarter\"] = pd.Categorical((seg1_test[\"ds\"].dt.month - 1) // 3)\n",
    "seg1_test[\"weekday\"] = pd.Categorical(seg1_test[\"ds\"].dt.weekday)\n",
    "onehot = pd.get_dummies(seg1_test[[\"day\", \"month\", \"quarter\", \"weekday\"]])\n",
    "seg1_test = pd.concat([seg1_test, onehot], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:48:44.089481Z",
     "start_time": "2020-01-24T17:48:42.104424Z"
    }
   },
   "outputs": [],
   "source": [
    "m.fit(seg1)\n",
    "fc_seg1 = m.predict(seg1_test)\n",
    "fc_seg1['yhat'] = np.clip(fc_seg1['yhat'], 0., fc_seg1['yhat'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:48:46.005128Z",
     "start_time": "2020-01-24T17:48:45.995176Z"
    }
   },
   "outputs": [],
   "source": [
    "fc_seg1 = fc_seg1[[\"ds\", \"yhat\"]]\n",
    "fc_seg1[\"segment\"] = 1\n",
    "fc_seg2 = fc_seg2[[\"ds\", \"yhat\"]]\n",
    "fc_seg2[\"segment\"] = 2\n",
    "fc = pd.concat([fc_seg1, fc_seg2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:48:48.354723Z",
     "start_time": "2020-01-24T17:48:48.350328Z"
    }
   },
   "outputs": [],
   "source": [
    "fc.rename(columns = {'ds': 'application_date', 'yhat': 'case_count'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:48:49.591006Z",
     "start_time": "2020-01-24T17:48:49.579158Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission_IIzFVsf.csv\", parse_dates = ['application_date'])\n",
    "sample_submission.drop(\"case_count\", axis = 1, inplace = True)\n",
    "sample_submission = sample_submission.merge(fc, on = ['segment', 'application_date'], how= 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T17:49:04.031127Z",
     "start_time": "2020-01-24T17:49:04.022592Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"../output/submission3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
